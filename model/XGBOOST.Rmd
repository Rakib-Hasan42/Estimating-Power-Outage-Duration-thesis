---
title: "XGBOOST"
author: "Rakib"
date: "2025-04-29"
output: html_document
---

#library
```{r}
library(dplyr)
library(caret)
library(scales)
```

#data
```{r}

data <- read.csv("C:/Users/rakib/Desktop/Thesis/DATA/combined final data set/final_data.csv")

data <- data%>%
  select(-c(date, fips_code))
```

#RFE
```{r}
# Define predictors (make sure to exclude target + IDs)
predictor_cols <- setdiff(names(data), c("total_outage", "county", 'event_name'))

set.seed(2000)
# Control setup
ctrl <- rfeControl(functions = caretFuncs, # use caret functions
                   method = "cv",       # cross-validation
                   number = 5)          # 5-fold CV


# Run RFE
rfe_model <- rfe(
  x = data[, predictor_cols],
  y = data$total_outage,
  sizes = c(16, 18, 20, 22, 23),         # test different feature set sizes
  rfeControl = ctrl,
  method = 'xgbTree'
)

#best subset
predictors(rfe_model)
#Plot Performance vs. Number of Features
plot(rfe_model, type = c("g", "o"))
```

#data split and selected variables
```{r}
#selected variables
selected_vars <- predictors(rfe_model)
set.seed(1)
# Split data into 80% training and 20% testing
train_index <- createDataPartition(data$total_outage, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
```

# XGBoost
```{r}
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(1002)

xgb_model <- train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)

print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage
# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 4),
    round(results["Rsquared"], 4),
    round(results["MAE"], 4)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```

# Idalia
```{r}
test_event <- c("idalia")  # for testing

train_data <- data %>%
  filter(!event_name %in% test_event)

test_data <- data %>%
  filter(event_name %in% test_event) 
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(200)

xgb_model <-  train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)
print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage

# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 2),
    round(results["Rsquared"], 4),
    round(results["MAE"], 2)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```

# Ian
```{r}
test_event <- c("ian")  # for testing

train_data <- data %>%
  filter(!event_name %in% test_event)

test_data <- data %>%
  filter(event_name %in% test_event) 
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(201)

xgb_model <-  train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)
print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage

# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 2),
    round(results["Rsquared"], 4),
    round(results["MAE"], 2)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```

# Nicole
```{r}
test_event <- c("nicole")  # for testing

train_data <- data %>%
  filter(!event_name %in% test_event)

test_data <- data %>%
  filter(event_name %in% test_event) 
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(202)

xgb_model <-  train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)
print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage

# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 2),
    round(results["Rsquared"], 4),
    round(results["MAE"], 2)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```

# Sally
```{r}
test_event <- c("sally")  # for testing

train_data <- data %>%
  filter(!event_name %in% test_event)

test_data <- data %>%
  filter(event_name %in% test_event) 
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(203)

xgb_model <-  train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)
print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage

# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 2),
    round(results["Rsquared"], 4),
    round(results["MAE"], 2)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```

# Eta
```{r}
test_event <- c("eta")  # for testing

train_data <- data %>%
  filter(!event_name %in% test_event)

test_data <- data %>%
  filter(event_name %in% test_event) 
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(204)

xgb_model <-  train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)
print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage

# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 2),
    round(results["Rsquared"], 4),
    round(results["MAE"], 2)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```

# Elsa
```{r}
test_event <- c("elsa")  # for testing

train_data <- data %>%
  filter(!event_name %in% test_event)

test_data <- data %>%
  filter(event_name %in% test_event) 
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(205)

xgb_model <-  train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)
print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage

# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 2),
    round(results["Rsquared"], 4),
    round(results["MAE"], 2)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```

# Mindy
```{r}
test_event <- c("mindy")  # for testing

train_data <- data %>%
  filter(!event_name %in% test_event)

test_data <- data %>%
  filter(event_name %in% test_event) 
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(206)

xgb_model <-  train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)
print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage

# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 2),
    round(results["Rsquared"], 4),
    round(results["MAE"], 2)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```

# Fred
```{r}
test_event <- c("fred")  # for testing

train_data <- data %>%
  filter(!event_name %in% test_event)

test_data <- data %>%
  filter(event_name %in% test_event) 
# Train control
ctrl <- trainControl(
  method = "cv", number = 5, verboseIter = TRUE
)

# Grid search space
xgb_grid <- expand.grid(
  nrounds = c(100, 300, 500),
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(4, 6, 8),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 1),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 1)
)
#train
set.seed(207)

xgb_model <-  train(
  x = train_data[, selected_vars],
  y = train_data$total_outage,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  metric = "RMSE"
)
print(xgb_model)
xgb_model$bestTune
xgb_model$finalModel
#Performance Metrics at Best Tune
best_row <- xgb_model$results[
  apply(xgb_model$results[, names(xgb_model$bestTune)], 1, function(x) all(x == xgb_model$bestTune)),
]
best_metrics <- best_row[, c("RMSE", "Rsquared", "MAE")]
print(best_metrics)
#variable imp
varImp(xgb_model, scale = FALSE)
plot(varImp(xgb_model, scale = FALSE))

#predict on test data
preds <- predict(xgb_model, newdata = test_data[, selected_vars])
preds[preds < 0] <- 0
actuals <- test_data$total_outage

# Evaluate
results <- postResample(pred = preds, obs = actuals)

# Format results as a data frame
eval_table <- data.frame(
  Metric = c("RMSE", "R-squared", "MAE"),
  Value = c(
    round(results["RMSE"], 2),
    round(results["Rsquared"], 4),
    round(results["MAE"], 2)
  )
)

# Print table
print(eval_table)
#Traditional/Definition based R2
sst <- sum((actuals - mean(actuals))^2)
sse <- sum((actuals - preds)^2)
r2_traditional <- 1 - (sse / sst)
cat("Traditional R-squared:", round(r2_traditional, 4))
#actual vs prediction plot
plot_df <- data.frame(
  actuals = actuals,
  preds = preds
)
ideal_line_df <- data.frame(x = c(0, max(plot_df$actuals)), y = c(0, max(plot_df$actuals)))

ggplot(plot_df, aes(x = actuals, y = preds)) +
  geom_point(aes(color = "Actual vs Predicted Points")) +
  geom_smooth(aes(color = "Regression Line"), method = "lm", se = FALSE) +
  geom_line(data = ideal_line_df, aes(x = x, y = y, color = "Ideal Line"), linetype = "dashed", size = 1) +
  scale_color_manual(
    name = "Legend",
    values = c(
      "Actual vs Predicted Points" = "blue",
      "Regression Line" = "green",
      "Ideal Line" = "red"
    )
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "Actual vs Predicted with Regression Line",
    x = "Actual Outage",
    y = "Predicted Outage",
    color = "Legend"
  ) +
  theme_minimal()
```
